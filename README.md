# üìö Telegram RAG Knowledge Base

A production-ready Retrieval-Augmented Generation (RAG) system that transforms Telegram chat exports into a searchable knowledge base with FastAPI interface.

[![Production Ready](https://img.shields.io/badge/Status-Production%20Ready-brightgreen)](https://github.com/maksdizzy/telegram-weaviate-rag)
[![Python 3.8+](https://img.shields.io/badge/Python-3.8%2B-blue)](https://python.org)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue)](https://docker.com)
[![FastAPI](https://img.shields.io/badge/FastAPI-Latest-green)](https://fastapi.tiangolo.com)

## ‚ú® Features

- üîç **Hybrid Search**: Combines semantic vector search with keyword matching
- üßµ **Smart Threading**: Automatically groups related messages into conversation threads
- ü§ñ **Multi-Provider Support**: Works with Ollama, OpenAI, and OpenRouter embeddings
- üöÄ **FastAPI REST API**: Production-ready API with authentication and error handling
- üì¶ **File Upload**: Upload multiple chat exports with merge functionality
- üîÑ **Incremental Updates**: Smart ingestion of new data without reprocessing
- üìä **Scalable**: Handles millions of messages efficiently with Weaviate
- üê≥ **Docker Ready**: Complete containerization with docker-compose
- üîí **Security**: Environment-based configuration, no hardcoded secrets

## üöÄ Quick Start

### Prerequisites

- **Docker & Docker Compose**
- **Python 3.8+** (for local development)
- **Telegram chat export** (JSON format)
- **At least 4GB RAM**
- **API key** (for OpenAI/OpenRouter) or **Ollama** (for local embeddings)

### One-Command Setup

```bash
# Clone the repository
git clone https://github.com/maksdizzy/telegram-weaviate-rag.git
cd telegram-weaviate-rag

# Copy and configure environment variables
cp .env.example .env
# Edit .env with your settings (see Configuration section)

# Run automated setup
./setup.sh

# Start the API server
python api.py
```

The setup script will:
- ‚úÖ Create Python virtual environment
- ‚úÖ Install all dependencies
- ‚úÖ Start Weaviate with Docker
- ‚úÖ Initialize database schema
- ‚úÖ Validate your configuration

## üìñ Table of Contents

- [Installation](#installation)
- [Configuration](#configuration)
- [Usage](#usage)
- [API Reference](#api-reference)
- [Architecture](#architecture)
- [Performance](#performance)
- [Troubleshooting](#troubleshooting)

## üì¶ Installation

### Option 1: Automated Setup (Recommended)

```bash
# Clone and enter directory
git clone https://github.com/maksdizzy/telegram-weaviate-rag.git
cd telegram-weaviate-rag

# Run the setup script
./setup.sh
```

### Option 2: Docker Compose Only

```bash
# Start all services
docker-compose up -d

# Check status
docker-compose ps

# View logs
docker-compose logs -f weaviate
```

### Option 3: Manual Local Development

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Start Weaviate
docker-compose up -d weaviate

# Configure environment
cp .env.example .env
# Edit .env file with your settings

# Initialize schema
python schema.py
```

## ‚öôÔ∏è Configuration

### Environment Variables

Create a `.env` file with your configuration:

```env
# Core Settings
API_KEY=your_secure_random_api_key_here
KNOWLEDGE_ID=telegram-rag
BATCH_SIZE=100

# Weaviate Configuration
WEAVIATE_HOST=localhost
WEAVIATE_PORT=8080
WEAVIATE_SCHEME=http

# Embedding Provider (choose one)
EMBEDDING_PROVIDER=openai  # Options: ollama, openai, openrouter

# OpenAI Configuration (recommended for production)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_EMBED_MODEL=text-embedding-3-small
OPENAI_GENERATION_MODEL=gpt-4-turbo-preview

# Ollama Configuration (for local/free embeddings)
OLLAMA_HOST=localhost
OLLAMA_PORT=11434
OLLAMA_EMBED_MODEL=nomic-embed-text
OLLAMA_GENERATION_MODEL=llama3.2

# OpenRouter Configuration (for multiple providers)
OPENROUTER_API_KEY=your_openrouter_api_key_here
OPENROUTER_EMBED_MODEL=openai/text-embedding-3-small
OPENROUTER_GENERATION_MODEL=anthropic/claude-3-haiku

# Thread Detection Settings
THREAD_TIME_WINDOW_MINUTES=5
THREAD_MIN_MESSAGES=1
THREAD_MAX_MESSAGES=50

# Search Settings
SEARCH_ALPHA=0.75  # 0=keyword only, 1=vector only
SEARCH_LIMIT=5
```

### Embedding Provider Comparison

| Provider | Models | Cost | Speed | Quality | Notes |
|----------|--------|------|-------|---------|--------|
| **OpenAI** | text-embedding-3-small/large | Paid | Fast | Excellent | Recommended for production |
| **Ollama** | nomic-embed-text, mxbai-embed-large | Free | Fast | Good | Great for local/private use |
| **OpenRouter** | Multiple providers | Paid | Variable | Variable | Good for experimentation |

### Setting Up Ollama (Free Local Option)

Ollama provides free, local embeddings without sending data to external services:

#### 1. Install Ollama

**On macOS:**
```bash
# Download from https://ollama.ai or use brew
brew install ollama
```

**On Linux:**
```bash
# Install script
curl -fsSL https://ollama.ai/install.sh | sh
```

**On Windows:**
```bash
# Download from https://ollama.ai/download/windows
# Or use winget
winget install Ollama.Ollama
```

#### 2. Start Ollama Service

```bash
# Start Ollama server
ollama serve

# Or start as background service
ollama serve &
```

#### 3. Download Required Models

```bash
# For embeddings (required)
ollama pull nomic-embed-text

# Alternative embedding model (higher quality, larger)
ollama pull mxbai-embed-large

# For text generation (optional)
ollama pull llama3.2

# Check downloaded models
ollama list
```

#### 4. Configure Environment

Update your `.env` file:
```env
EMBEDDING_PROVIDER=ollama
OLLAMA_HOST=localhost
OLLAMA_PORT=11434
OLLAMA_EMBED_MODEL=nomic-embed-text
OLLAMA_GENERATION_MODEL=llama3.2
```

#### 5. Test Ollama Setup

```bash
# Test if Ollama is running
curl http://localhost:11434/api/tags

# Test embedding generation
ollama run nomic-embed-text "test embedding"

# Verify in our system
python config.py
```

#### Recommended Ollama Models

**For Embeddings:**
- `nomic-embed-text` (274MB) - Fast, good quality, recommended
- `mxbai-embed-large` (669MB) - Higher quality, slower
- `all-minilm:l6-v2` (91MB) - Fastest, lower quality

**For Text Generation:**
- `llama3.2` (2GB) - Good balance of speed and quality
- `llama3.2:1b` (1.3GB) - Fastest, smaller model
- `mistral` (4.1GB) - Higher quality, slower

#### Troubleshooting Ollama

**Ollama Not Starting:**
```bash
# Check if service is running
ps aux | grep ollama

# Restart Ollama
pkill ollama
ollama serve

# Check logs
ollama logs
```

**Model Download Issues:**
```bash
# Clear cache and retry
rm -rf ~/.ollama/models/blobs/*
ollama pull nomic-embed-text

# Check available disk space
df -h ~/.ollama
```

**Memory Issues:**
```bash
# Use smaller models for limited RAM
ollama pull nomic-embed-text  # Uses ~1GB RAM
# Instead of mxbai-embed-large  # Uses ~2GB RAM
```

## üìñ Usage

### 1. Prepare Your Telegram Data

Export your Telegram chat:
1. Open **Telegram Desktop**
2. Go to **Settings ‚Üí Advanced ‚Üí Export Telegram Data**
3. Select **JSON** format
4. Choose your chat(s)
5. Save as `result.json` in the project directory

### 2. Upload and Process Data

#### Option A: File Upload API
```bash
# Upload your first chat export
curl -X POST "http://localhost:8000/upload" \
  -H "Authorization: Bearer your_api_key" \
  -F "file=@result.json" \
  -F "chat_name=MyMainChat"

# Upload additional chats with merge
curl -X POST "http://localhost:8000/upload" \
  -H "Authorization: Bearer your_api_key" \
  -F "file=@another_chat.json" \
  -F "chat_name=WorkChat" \
  -F "merge=true"
```

#### Option B: Manual Processing
```bash
# Process messages into conversation threads
python thread_detector.py

# Ingest into Weaviate vector database
python ingestion.py

# For incremental updates
python ingestion.py --incremental
```

### 3. Start the API Server

```bash
# Start the FastAPI server
python api.py

# Custom port
python api.py --port 8001

# API will be available at http://localhost:8000
# API documentation at http://localhost:8000/docs
```

### 4. Search Your Knowledge Base

#### Interactive Testing
```bash
# Test search functionality
python test_rag.py

# Run search tests
python test_search.py
```

#### API Usage
```bash
# Search via API
curl -X POST "http://localhost:8000/retrieval" \
  -H "Authorization: Bearer your_api_key" \
  -H "Content-Type: application/json" \
  -d '{
    "knowledge_id": "telegram-rag",
    "query": "What was discussed about the project?",
    "retrieval_setting": {
      "top_k": 5,
      "score_threshold": 0.5
    }
  }'
```

## üîå API Reference

### Authentication

All API endpoints require Bearer token authentication:

```bash
Authorization: Bearer your_api_key_from_env
```

### Endpoints

#### POST `/retrieval`
Retrieve relevant content from the knowledge base.

**Request:**
```json
{
  "knowledge_id": "telegram-rag",
  "query": "search query",
  "retrieval_setting": {
    "top_k": 5,
    "score_threshold": 0.5
  }
}
```

**Response:**
```json
{
  "records": [
    {
      "content": "[Thread with Alice, Bob - 3 messages]\n[2024-01-01 10:00:00] Alice: Hello...",
      "score": 0.95,
      "metadata": {
        "thread_id": "thread_20240101_100000",
        "participants": ["Alice", "Bob"],
        "message_count": 3,
        "timestamp": "2024-01-01T10:00:00Z"
      }
    }
  ]
}
```

#### POST `/upload`
Upload Telegram chat export files.

**Form Data:**
- `file`: JSON file (required)
- `chat_name`: Chat identifier (optional)
- `merge`: Merge with existing data (optional, default: false)

#### POST `/ingest`
Trigger incremental data ingestion.

**Request:**
```json
{
  "incremental": true
}
```

#### GET `/health`
Check API and database health.

#### GET `/stats`
Get knowledge base statistics.

### Error Handling

The API returns structured error responses:

```json
{
  "detail": {
    "error_code": 1002,
    "error_message": "Authorization failed"
  }
}
```

Common error codes:
- `1002`: Authorization failed
- `1003`: Query cannot be empty
- `2001`: Knowledge base does not exist
- `4001-4003`: File upload errors
- `5000`: Internal server error

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Telegram JSON  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇThread Detector‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Documents  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                     ‚îÇ
                                                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Client/Dify   ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ   FastAPI    ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Weaviate   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ   Server     ‚îÇ     ‚îÇ  Database   ‚îÇ
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                     ‚ñ≤
                                                     ‚îÇ
                                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                              ‚îÇ  Embedding  ‚îÇ
                                              ‚îÇ  Providers  ‚îÇ
                                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Components

- **Thread Detector**: Groups related messages using time windows and reply chains
- **Embedding Providers**: Abstract interface supporting multiple AI services
- **FastAPI Server**: Production-ready REST API with authentication
- **Weaviate Database**: Vector database with hybrid search capabilities
- **Data Models**: Pydantic V2 models for validation and serialization

## üìä Performance

### Benchmarks
- **Thread Detection**: ~1000 messages/second
- **Embedding Generation**:
  - OpenAI: ~50 docs/second
  - Ollama: ~30 docs/second
- **Search Latency**: <200ms
- **API Response**: <500ms
- **Storage Efficiency**: ~10MB per 1000 threads
- **Memory Usage**: 2-4GB during ingestion

### Optimization Tips
- Use `text-embedding-3-small` for best speed/quality balance
- Increase `BATCH_SIZE` for faster ingestion (up to 200)
- Use SSD storage for better Weaviate performance
- Allocate 8GB+ RAM for large datasets (100k+ messages)

## üîß Scripts and Tools

### Management Scripts
```bash
# Setup everything
./setup.sh

# Stop all services
./stop.sh

# Check system readiness
python quickstart_check_readiness.py

# Clear all data
python clear_data.py
```

### Development Tools
```bash
# Validate configuration
python config.py

# Manual schema setup
python schema.py

# Test RAG functionality
python test_rag.py

# Check system readiness
python quickstart_check_readiness.py
```

## üêõ Troubleshooting

### Common Issues

#### 1. Weaviate Connection Error
```bash
# Check if Weaviate is running
docker ps | grep weaviate

# Restart Weaviate
docker-compose restart weaviate

# Check logs
docker-compose logs weaviate
```

#### 2. API Authentication Fails
```bash
# Verify API key in .env
cat .env | grep API_KEY

# Generate new API key
openssl rand -hex 32
```

#### 3. Embedding Provider Issues

**For Ollama:**
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# Start Ollama if not running
ollama serve

# Pull required embedding model
ollama pull nomic-embed-text

# List available models
ollama list

# Test model directly
ollama run nomic-embed-text "test text"

# Check system resources
ollama ps
```

**For OpenAI/OpenRouter:**
- Verify API key is valid
- Check account credits/usage limits
- Ensure correct model names
- Test API connectivity:
```bash
curl -H "Authorization: Bearer your_api_key" \
  https://api.openai.com/v1/models
```

#### 4. Slow Performance
- Reduce `BATCH_SIZE` in .env
- Use faster embedding models
- Increase Docker memory allocation
- Use SSD storage

#### 5. Memory Issues
```bash
# For large datasets, increase system memory
# Or process in smaller batches
export BATCH_SIZE=50
python ingestion.py
```

### Debug Mode

Enable debug logging:
```bash
export LOG_LEVEL=DEBUG
python api.py
```

## ü§ù Contributing

We welcome contributions! Areas where help is needed:

- **New embedding providers** (Cohere, Hugging Face, etc.)
- **Performance optimizations**
- **Additional chat export formats** (WhatsApp, Discord, etc.)
- **UI/Frontend development**
- **Documentation improvements**

Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## üìÑ License

MIT License - see [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- Built with [Weaviate](https://weaviate.io/) vector database
- Compatible with [Dify](https://dify.ai/) for AI workflow automation
- Supports [Ollama](https://ollama.ai/), OpenAI, and OpenRouter
- Inspired by the open-source RAG community

## üìû Support

- üêõ **Issues**: [GitHub Issues](https://github.com/maksdizzy/telegram-weaviate-rag/issues)
- üí¨ **Discussions**: [GitHub Discussions](https://github.com/maksdizzy/telegram-weaviate-rag/discussions)
- üìß **Email**: maxim.a.savelyev@gmail.com

---

**Made with ‚ù§Ô∏è for the RAG community**

‚≠ê **If this project helps you, please give it a star!** ‚≠ê